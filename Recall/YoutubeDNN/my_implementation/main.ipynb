{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD USER AND DOC INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "DATA_DIR = \"/Users/hanshen/work/AI-RecommenderSystem/Dataset/news_data_bigger\"\n",
    "\n",
    "user_info_cols = [\"userid\", \"device\", \"operating_system\", \"province\", \"city\", \"age\", \"gender\"]\n",
    "doc_info_cols = [\"docid\", \"title\", \"create_time\", \"image_num\", \"cate1\", \"cate2\", \"keywords\"]\n",
    "show_info_cols = [\"userid\", \"docid\", \"exp_time\", \"network\", \"rt\", \"rit\", \"click\", \"reading_time\"]\n",
    "\n",
    "UserInfo = namedtuple(\"Userinfo\", user_info_cols)\n",
    "\n",
    "DocInfo = namedtuple(\"DocInfo\", doc_info_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_create_time(value):\n",
    "    if len(value) == 0:\n",
    "        return np.uint32(0)\n",
    "    return np.uint32(value)\n",
    "\n",
    "def clean_image_num(value):\n",
    "    if len(value) == 0:\n",
    "        return np.uint8(0)\n",
    "    return np.uint8(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_info = pd.read_csv(\n",
    "    \"/Users/hanshen/work/AI-RecommenderSystem/Dataset/news_data_bigger/user_info.txt\",\n",
    "    sep=\"\\t\", header=None, names=user_info_cols,\n",
    "    dtype=str,\n",
    "    keep_default_na=False\n",
    ")\n",
    "\n",
    "doc_info = pd.read_csv(\n",
    "    \"/Users/hanshen/work/AI-RecommenderSystem/Dataset/news_data_bigger/doc_info.txt\",\n",
    "    sep=\"\\t\", header=None, names=doc_info_cols,\n",
    "    converters={\n",
    "        \"create_time\": clean_create_time,\n",
    "        \"image_num\": clean_image_num,\n",
    "    },\n",
    "    dtype={\n",
    "        \"docid\": str\n",
    "    }\n",
    ")\n",
    "\n",
    "show_info = pd.read_csv(\n",
    "    \"/Users/hanshen/work/AI-RecommenderSystem/Dataset/news_data_bigger/sorted_train_data.txt\",\n",
    "    sep=\"\\t\", names=show_info_cols,\n",
    "    dtype=str,\n",
    "    keep_default_na=False,\n",
    "    nrows=1000000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_proba(row: str):\n",
    "    '''Suppose row has the following format: key1:[float],key2:[float]\n",
    "    '''\n",
    "    if not isinstance(row, str):\n",
    "        return \"UNK\"\n",
    "    if len(row) == 0:\n",
    "        return \"UNK\"\n",
    "    classes = row.split(\",\")\n",
    "    assert len(classes) >= 1, \"unkown format: [{}]\".format(row)\n",
    "    max_proba = 0\n",
    "    max_class = \"UNK\"\n",
    "    for cls_pair in classes:\n",
    "        cls, proba = cls_pair.split(\":\")\n",
    "        if float(proba) > max_proba:\n",
    "            max_class = cls\n",
    "            max_proba = float(proba)\n",
    "    return max_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_info[\"age\"] = user_info[\"age\"].apply(reduce_proba)\n",
    "user_info[\"gender\"] = user_info[\"gender\"].apply(reduce_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_info_dict: Dict[int, UserInfo] = {}\n",
    "doc_info_dict: Dict[int, DocInfo] = {}\n",
    "\n",
    "for row in user_info.iterrows():\n",
    "    user_info_dict[row[1][\"userid\"]] = UserInfo(*(row[1]))\n",
    "\n",
    "for row in doc_info.iterrows():\n",
    "    doc_info_dict[row[1][\"docid\"]] = DocInfo(*(row[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DocInfo(docid='349635709', title='拿到c1驾照后,实习期扣分了会怎样?扣12分驾照会吊销么?', create_time=561940664, image_num=9, cate1='汽车', cate2='汽车/用车', keywords='上班族:8.469502,买车:8.137443,二手车:9.022247,副页:11.218712,国人:5.104467,大学生:7.731338,家庭:6.529803,家用车:7.034796,标志:9.054356,汽车:7.582007,注意^^事项:7.826521,独立:7.015873,行业:6.600394,车主:9.498086,车尾:5.677475,违章行为:9.028495,驾照^^实习期:7.906046,驾考:11.347330,驾驶证:9.018305,驾驶证^^副页:7.525696')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_info_dict[\"349635709\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODIFY DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs = {\n",
    "    \"userid\": user_info.userid.unique(),\n",
    "    \"device\": user_info.device.unique(),\n",
    "    \"operating_system\": user_info.operating_system.unique(),\n",
    "    \"province\": user_info.province.unique(),\n",
    "    \"city\": user_info.city.unique(),\n",
    "    \"age\": user_info.age.unique(),\n",
    "    \"gender\": user_info.gender.unique(),\n",
    "    \"docid\": doc_info.docid.unique(),\n",
    "    \"network\": show_info.network.unique(),\n",
    "    \"rt\": show_info.rt.unique(),\n",
    "    \"rit\": show_info.rit.unique(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying to create userid StringLookup layer...\n",
      "trying to create device StringLookup layer...\n",
      "trying to create operating_system StringLookup layer...\n",
      "trying to create province StringLookup layer...\n",
      "trying to create city StringLookup layer...\n",
      "trying to create age StringLookup layer...\n",
      "trying to create gender StringLookup layer...\n",
      "trying to create docid StringLookup layer...\n",
      "trying to create network StringLookup layer...\n",
      "trying to create rt StringLookup layer...\n",
      "trying to create rit StringLookup layer...\n"
     ]
    }
   ],
   "source": [
    "converter_layers: Dict[str, keras.layers.StringLookup] = {}\n",
    "\n",
    "for key, vocab in vocabs.items():\n",
    "    if os.path.exists(\"./{}.pkl\".format(key)):\n",
    "        print(\"trying to load {} StringLookup layer...\".format(key))\n",
    "        from_disk = pickle.load(open(\"./{}.pkl\".format(key), \"rb\"))\n",
    "        new_layer = keras.layers.StringLookup().from_config(from_disk[\"config\"])\n",
    "        new_layer.adapt(tf.data.Dataset.from_tensor_slices([\"xyz\"]))\n",
    "        new_layer.set_weights(from_disk['weights'])\n",
    "    else:\n",
    "        print(\"trying to create {} StringLookup layer...\".format(key))\n",
    "        new_layer = keras.layers.StringLookup(num_oov_indices=1)\n",
    "        new_layer.adapt(data=vocabs[key])\n",
    "        pickle.dump({\n",
    "            \"config\": new_layer.get_config(),\n",
    "            \"weights\": new_layer.get_weights(),\n",
    "        }, open(\"./{}.pkl\".format(key), \"wb\"))\n",
    "    converter_layers[key] = new_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## click sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_log_cols = [\"userid\", \"docid\", \"exp_time\", \"network\", \"rt\", \"rit\", \"click\", \"reading_time\"]\n",
    "\n",
    "ShowLog = namedtuple(\"Userinfo\", [\"userid\", \"docid\", \"exp_time\", \"network\", \"rt\", \"rit\", \"click\", \"reading_time\"])\n",
    "\n",
    "SEQ_LENGTH = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make seq file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import deque\n",
    "\n",
    "# SEQ_LENGTH = 20\n",
    "\n",
    "# user_seq_buffer: Dict[str, deque] = {}\n",
    "\n",
    "# with open(\"/Users/hanshen/work/AI-RecommenderSystem/Dataset/news_data_bigger/sorted_train_data.txt\", \"r\") as show_log_f, open(\"/Users/hanshen/work/AI-RecommenderSystem/Dataset/news_data_bigger/clk_seq_from_sorted_train_data.txt\", 'w') as seq_f:\n",
    "#     for line in show_log_f:\n",
    "#         parts = line[:-1].split(\"\\t\")\n",
    "#         show_log = ShowLog(*parts)\n",
    "#         if show_log.userid not in user_seq_buffer:\n",
    "#             user_seq_buffer[show_log.userid] = deque(maxlen=SEQ_LENGTH)\n",
    "#         dq_of_this_user = user_seq_buffer[show_log.userid]\n",
    "#         seq_str = \" \".join(dq_of_this_user)\n",
    "#         seq_f.write(\",\".join(show_log[:3] + (seq_str,)) + \"\\n\")\n",
    "#         if show_log.click == \"1\":\n",
    "#             dq_of_this_user.append(show_log.docid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter_layers[\"docid\"](tf.constant([\"\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seq_feat(ele):\n",
    "    ele = tf.strings.split(ele, \",\").to_tensor()\n",
    "    # tf.print(\"1\", ele, tf.shape(ele))\n",
    "    ele = ele[:, 3]\n",
    "    # tf.print(\"2\", ele, tf.shape(ele))\n",
    "    ele = tf.strings.split(ele, \" \").to_tensor()\n",
    "    # tf.print(\"3\", ele, tf.shape(ele))\n",
    "    res = converter_layers[\"docid\"](ele)\n",
    "    # tf.print(res, type(res), tf.shape(res))\n",
    "    return res\n",
    "\n",
    "seq_dataset = tf.data.TextLineDataset([DATA_DIR + \"/clk_seq_from_sorted_train_data.txt\"])\\\n",
    "    .batch(1024, drop_remainder=True)\\\n",
    "    .map(get_seq_feat)\n",
    "    # .unbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in seq_dataset.skip(10000).take(2):\n",
    "    # print(e)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## other feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make feat by pythonic method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用户id、文章id、展现时间、网路环境、刷新次数、展现位置、是否点击、消费时长（秒）；\n",
    "\n",
    "\n",
    "def py_get_user_feat(uid):\n",
    "    # print(uid.numpy(), type(uid.numpy()))\n",
    "    user_info = user_info_dict[uid.numpy().decode()]  # Because of this dict access, we cannot do the tensorflow style pipeline\n",
    "    res = []\n",
    "    for k, v in zip(user_info._fields, user_info):\n",
    "        res.append(converter_layers[k](v))\n",
    "    return res\n",
    "\n",
    "\n",
    "def tf_get_feat(ele: tf.Tensor):\n",
    "    ele: tf.Tensor = tf.strings.split(ele, \"\\t\")\n",
    "    user_info = tf.py_function(py_get_user_feat, [ele[0]], [tf.int64] * len(user_info_cols))\n",
    "    network = converter_layers[\"network\"](ele[3])\n",
    "    rt = converter_layers[\"rt\"](ele[4])\n",
    "    rit = converter_layers[\"rit\"](ele[5])\n",
    "    feat = tf.concat([user_info] + [[network], [rt], [rit]], axis=0)\n",
    "    return feat\n",
    "\n",
    "train_show_log = tf.data.TextLineDataset([\"/Users/hanshen/work/AI-RecommenderSystem/Dataset/news_data_bigger/sorted_train_data.txt\"])\\\n",
    "    .map(tf_get_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.57 s ± 47.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "for e in train_show_log.take(1000):\n",
    "    # print(e)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build MutableHashTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用户id、设备名称、操作系统、所在省、所在市、年龄、性别；\n",
    "\n",
    "user_info_table = tf.lookup.experimental.MutableHashTable(\n",
    "    key_dtype=tf.int64,\n",
    "    value_dtype=tf.string,\n",
    "    default_value=[\"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\"],\n",
    " )\n",
    "\n",
    "\n",
    "def build_hash_table(ele):\n",
    "    # tf.print(ele[:, 0], type(ele), ele.shape)\n",
    "    key = tf.strings.to_number([ele[:, 0]], out_type=tf.int64)\n",
    "    # tf.print(key, type(key), key.shape)\n",
    "    # tf.print(key)\n",
    "    user_info_table.insert(key, tf.expand_dims(ele, axis=0))\n",
    "    return 1\n",
    "\n",
    "\n",
    "user_info_ = tf.data.Dataset.from_tensor_slices(user_info)\\\n",
    "    .batch(1000)\\\n",
    "    .map(build_hash_table)\n",
    "\n",
    "\n",
    "\n",
    "for e in user_info_:\n",
    "    # print(e)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 7), dtype=string, numpy=\n",
       "array([[b'1001384888', b'M2007J22C', b'Android',\n",
       "        b'\\xe6\\xb2\\xb3\\xe5\\x8c\\x97',\n",
       "        b'\\xe7\\x9f\\xb3\\xe5\\xae\\xb6\\xe5\\xba\\x84', b'A_40+', b'male']],\n",
       "      dtype=object)>"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_info_table.lookup(tf.constant([1001384888], dtype=tf.int64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make feat by tf method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用户id、文章id、展现时间、网路环境、刷新次数、展现位置、是否点击、消费时长（秒）；\n",
    "\n",
    "def tf_get_feat_from_table(ele: tf.Tensor):\n",
    "    ele: tf.Tensor = tf.strings.split(ele, \"\\t\").to_tensor()\n",
    "    uids = tf.strings.to_number([ele[:, 0]], out_type=tf.int64)\n",
    "    label = tf.strings.to_number([ele[:, 6]], out_type=tf.int64)\n",
    "    label = tf.reshape(label, shape=[-1, 1])  # [1, batch] to [batch, 1]\n",
    "    # tf.print(label, type(label), tf.shape(label))\n",
    "    values = user_info_table.lookup(uids)\n",
    "    values = tf.squeeze(values, axis=0)\n",
    "    # tf.print(values, tf.shape(values))\n",
    "    feat_dict = {}\n",
    "    for idx, key in enumerate(user_info_cols):\n",
    "        tmp = converter_layers[key](values[:, idx])\n",
    "        feat_dict[key] = tmp\n",
    "    # user_feat = tf.stack(user_feat, axis=1)\n",
    "    # tf.print(user_feat)\n",
    "    docid = converter_layers[\"docid\"](ele[:, 1])\n",
    "    feat_dict[\"docid\"] = docid\n",
    "    network = converter_layers[\"network\"](ele[:, 3])\n",
    "    feat_dict[\"network\"] = network\n",
    "    rt = converter_layers[\"rt\"](ele[:, 4])\n",
    "    feat_dict[\"rt\"] = rt\n",
    "    rit = converter_layers[\"rit\"](ele[:, 5])\n",
    "    feat_dict[\"rit\"] = rit\n",
    "    # tf.print(network, tf.shape(network))\n",
    "    # context_feat = tf.stack([network, rt, rit], axis=1)\n",
    "    # tf.print(context_feat, tf.shape(context_feat))\n",
    "    # feat = tf.stack(user_feat + [network, rt, rit], axis=1)\n",
    "    # tf.print(feat)\n",
    "    return (feat_dict, label)\n",
    "\n",
    "\n",
    "\n",
    "train_show_log = tf.data.TextLineDataset([\"/Users/hanshen/work/AI-RecommenderSystem/Dataset/news_data_bigger/sorted_train_data.txt\"])\\\n",
    "    .batch(1024, drop_remainder=True)\\\n",
    "    .map(tf_get_feat_from_table)\n",
    "    # .unbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'userid': <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 235691,  528795, 1248234, ...,  947192, 1472675, 1472675])>, 'device': <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([1842, 2236, 1367, ..., 1137,   96,   96])>, 'operating_system': <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([2, 2, 2, ..., 2, 2, 2])>, 'province': <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 14,  69, 105, ...,  52,  84,  84])>, 'city': <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([295, 179, 228, ..., 256, 113, 113])>, 'age': <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([2, 3, 2, ..., 4, 2, 2])>, 'gender': <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([1, 1, 1, ..., 1, 1, 1])>, 'docid': <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([581525, 562325, 583990, ..., 518838, 530587, 549283])>, 'network': <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([1, 4, 4, ..., 4, 4, 4])>, 'rt': <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([214, 214, 102, ..., 214, 213, 213])>, 'rit': <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 923,  923,  723, ...,  923,  934, 1013])>}\n"
     ]
    }
   ],
   "source": [
    "# %%timeit\n",
    "\n",
    "for e in train_show_log.take(1):\n",
    "    print(e[0])\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### concat show log and req feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sparse_with_seq_into_one_dict(a, seq):\n",
    "    sparse, label = a\n",
    "    for _, value in sparse.items():\n",
    "        value.set_shape([1024])\n",
    "    sparse[\"docid_seq\"] = seq\n",
    "    return (sparse, label)\n",
    "\n",
    "dataset = tf.data.Dataset.zip((train_show_log, seq_dataset)).map(merge_sparse_with_seq_into_one_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'userid': <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 235691,  528795, 1248234, ...,  947192, 1472675, 1472675])>, 'device': <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([1842, 2236, 1367, ..., 1137,   96,   96])>, 'operating_system': <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([2, 2, 2, ..., 2, 2, 2])>, 'province': <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 14,  69, 105, ...,  52,  84,  84])>, 'city': <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([295, 179, 228, ..., 256, 113, 113])>, 'age': <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([2, 3, 2, ..., 4, 2, 2])>, 'gender': <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([1, 1, 1, ..., 1, 1, 1])>, 'docid': <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([581525, 562325, 583990, ..., 518838, 530587, 549283])>, 'network': <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([1, 4, 4, ..., 4, 4, 4])>, 'rt': <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([214, 214, 102, ..., 214, 213, 213])>, 'rit': <tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 923,  923,  723, ...,  923,  934, 1013])>, 'docid_seq': <tf.Tensor: shape=(1024, 5), dtype=int64, numpy=\n",
      "array([[0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0]])>}, <tf.Tensor: shape=(1024, 1), dtype=int64, numpy=\n",
      "array([[0],\n",
      "       [1],\n",
      "       [1],\n",
      "       ...,\n",
      "       [0],\n",
      "       [0],\n",
      "       [1]])>)\n"
     ]
    }
   ],
   "source": [
    "for e in dataset.take(1):\n",
    "    print(e)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset, test_dataset = tf.keras.utils.split_dataset(dataset.take(1_000_000), left_size=0.9)\n",
    "# dataset has approximately 185319 batches.\n",
    "\n",
    "train_dataset = dataset.take(100_000)\n",
    "test_dataset = dataset.skip(100_000).take(30_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make negative sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant([\"1\"]) == tf.constant([\"1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'2382994490\\t462336897\\t1624546614290\\t2\\t0\\t16\\t1\\t132', shape=(), dtype=string)\n",
      "tf.Tensor(b'1499193026\\t461957659\\t1624546617762\\t2\\t2\\t34\\t1\\t192', shape=(), dtype=string)\n",
      "tf.Tensor(b'2226310380\\t462616014\\t1624546621702\\t5\\t1\\t14\\t1\\t718', shape=(), dtype=string)\n",
      "tf.Tensor(b'2230458112\\t462830838\\t1624546621712\\t5\\t0\\t13\\t1\\t163', shape=(), dtype=string)\n",
      "tf.Tensor(b'2216481992\\t462753067\\t1624546621919\\t5\\t0\\t12\\t1\\t386', shape=(), dtype=string)\n",
      "tf.Tensor(b'2439786870\\t462579914\\t1624546626211\\t2\\t0\\t11\\t1\\t141', shape=(), dtype=string)\n",
      "tf.Tensor(b'2391075066\\t462456982\\t1624546638325\\t5\\t2\\t14\\t1\\t43', shape=(), dtype=string)\n",
      "tf.Tensor(b'2391075066\\t462573736\\t1624546638325\\t5\\t2\\t19\\t1\\t217', shape=(), dtype=string)\n",
      "tf.Tensor(b'1499904554\\t461849959\\t1624546648071\\t2\\t12\\t7\\t1\\t99', shape=(), dtype=string)\n",
      "tf.Tensor(b'1251029748\\t462861190\\t1624546652702\\t2\\t0\\t13\\t1\\t151', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# 用户id、文章id、展现时间、网路环境、刷新次数、展现位置、是否点击、消费时长（秒）；\n",
    "\n",
    "# doc_click_freq = tf.lookup.experimental.MutableHashTable(\n",
    "#     key_dtype=tf.int64,\n",
    "#     value_dtype=tf.int64,\n",
    "#     default_value=0,\n",
    "# )\n",
    "\n",
    "\n",
    "# def split_ele(ele):\n",
    "#     return tf.strings.split(ele, \"\\t\")\n",
    "\n",
    "\n",
    "# def is_click(ele):\n",
    "#     res = (ele[6] == tf.constant([\"1\"]))\n",
    "#     # tf.print(res[0], type(res[0]))\n",
    "#     return res[0]\n",
    "\n",
    "\n",
    "# click_log_dataset = tf.data.TextLineDataset(DATA_DIR + \"/sorted_train_data.txt\")\\\n",
    "#     .map(split_ele)\n",
    "#     .filter(is_click)\n",
    "\n",
    "# for e in click_log_dataset.take(10):\n",
    "#     print(e)\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = tf.random.categorical(tf.math.log([[0.7, 0.3]]), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int64, numpy=27>"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import model\n",
    "importlib.reload(model)\n",
    "\n",
    "sparse_configs = []\n",
    "doc_embedding = None\n",
    "\n",
    "for key, layer in converter_layers.items():\n",
    "    if key == \"docid\":\n",
    "        doc_embedding = model.EmbeddingConfig(key, 16, layer.vocabulary_size())\n",
    "        continue\n",
    "    sparse_configs.append(\n",
    "        model.EmbeddingConfig(key, 16, layer.vocabulary_size())\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = \"./ckpt/{epoch:02d}-{val_auc:.2f}.hdf5\"\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor=\"val_auc\",\n",
    "    mode=\"max\",\n",
    "    save_best_only=True,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "log_dir = \"./logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtubednn = model.YouTubeDNN(sparse_configs, doc_embedding)\n",
    "youtubednn.compile(\n",
    "    optimizer='adam',\n",
    "    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=keras.metrics.AUC(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['final_bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['final_bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      " 100000/Unknown - 21020s 210ms/step - loss: 0.3607 - auc_21: 0.5197"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Failed to format this callback filepath: \"./ckpt/{epoch:02d}-{val_auc:.2f}.hdf5\". Reason: \\'val_auc\\''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/hanshen/work/AI-RecommenderSystem/Recall/YoutubeDNN/my_implementation/main.ipynb Cell 46\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/hanshen/work/AI-RecommenderSystem/Recall/YoutubeDNN/my_implementation/main.ipynb#Y126sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m youtubednn\u001b[39m.\u001b[39mfit(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hanshen/work/AI-RecommenderSystem/Recall/YoutubeDNN/my_implementation/main.ipynb#Y126sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     x \u001b[39m=\u001b[39m train_dataset,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hanshen/work/AI-RecommenderSystem/Recall/YoutubeDNN/my_implementation/main.ipynb#Y126sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     validation_data\u001b[39m=\u001b[39mtest_dataset,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hanshen/work/AI-RecommenderSystem/Recall/YoutubeDNN/my_implementation/main.ipynb#Y126sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     callbacks\u001b[39m=\u001b[39m[\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hanshen/work/AI-RecommenderSystem/Recall/YoutubeDNN/my_implementation/main.ipynb#Y126sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         model_checkpoint_callback,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hanshen/work/AI-RecommenderSystem/Recall/YoutubeDNN/my_implementation/main.ipynb#Y126sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         tensorboard_callback,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hanshen/work/AI-RecommenderSystem/Recall/YoutubeDNN/my_implementation/main.ipynb#Y126sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     ]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hanshen/work/AI-RecommenderSystem/Recall/YoutubeDNN/my_implementation/main.ipynb#Y126sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m )\n",
      "File \u001b[0;32m~/work/.venv/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/work/.venv/lib/python3.9/site-packages/keras/callbacks.py:1588\u001b[0m, in \u001b[0;36mModelCheckpoint._get_file_path\u001b[0;34m(self, epoch, batch, logs)\u001b[0m\n\u001b[1;32m   1584\u001b[0m         file_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilepath\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1585\u001b[0m             epoch\u001b[39m=\u001b[39mepoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, batch\u001b[39m=\u001b[39mbatch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mlogs\n\u001b[1;32m   1586\u001b[0m         )\n\u001b[1;32m   1587\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m-> 1588\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\n\u001b[1;32m   1589\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFailed to format this callback filepath: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilepath\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1590\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReason: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1591\u001b[0m     )\n\u001b[1;32m   1592\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_write_filepath \u001b[39m=\u001b[39m distributed_file_utils\u001b[39m.\u001b[39mwrite_filepath(\n\u001b[1;32m   1593\u001b[0m     file_path, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mdistribute_strategy\n\u001b[1;32m   1594\u001b[0m )\n\u001b[1;32m   1595\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_write_filepath\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Failed to format this callback filepath: \"./ckpt/{epoch:02d}-{val_auc:.2f}.hdf5\". Reason: \\'val_auc\\''"
     ]
    }
   ],
   "source": [
    "youtubednn.fit(\n",
    "    x = train_dataset,\n",
    "    validation_data=test_dataset,\n",
    "    callbacks=[\n",
    "        model_checkpoint_callback,\n",
    "        tensorboard_callback,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[1, 4, 9]], dtype=int32)>"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant([[1,2,3]]) * tf.constant([[1,2,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[14]], dtype=int32)>"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.tensordot(tf.constant([[1,2,3]]), tf.constant([[1,2,3]]), axes=[[1], [1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
